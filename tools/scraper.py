import requests
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import time
import re
from typing import List, Dict, Any, Optional
import json

class WebScraper:
    """ç½‘é¡µæŠ“å–å·¥å…·"""
    
    def __init__(self):
        self.ua = UserAgent()
        self.headers = {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
    def fetch_page(self, url: str, timeout: int = 30, params: Optional[Dict] = None) -> str:
        """æŠ“å–ç½‘é¡µå†…å®¹"""
        try:
            response = requests.get(url, headers=self.headers, timeout=timeout, params=params)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            return response.text
        except Exception as e:
            print(f"æŠ“å–å¤±è´¥ {url}: {e}")
            return ""
    
    def extract_employment_data(self, html: str) -> Dict[str, Any]:
        """ä»HTMLä¸­æå–å°±ä¸šæ•°æ®"""
        soup = BeautifulSoup(html, 'lxml')
        data = {
            'total_graduates': 0,
            'employment_rate': 0,
            'signing_rate': 0,
            'province': '',
            'school_type': '',
            'major_categories': {},
            'freelance_data': {},
            'trends': []
        }
        
        # æå–æ–‡æœ¬å†…å®¹
        text = soup.get_text(separator=' ', strip=True)
        
        # åŒ¹é…æ¯•ä¸šäººæ•°
        graduate_patterns = [
            r'æ¯•ä¸š[ç”Ÿäººæ•°]+[:ï¼š]?(\d+[ä¸‡åƒä¸‡]?)äºº',
            r'(\d+[ä¸‡åƒä¸‡]?)æ¯•ä¸šç”Ÿ',
            r'å…±(\d+[ä¸‡åƒä¸‡]?)åæ¯•ä¸šç”Ÿ'
        ]
        for pattern in graduate_patterns:
            match = re.search(pattern, text)
            if match:
                data['total_graduates'] = match.group(1)
                break
        
        # åŒ¹é…å°±ä¸šç‡
        rate_patterns = [
            r'å°±ä¸šç‡[:ï¼š]?(\d+\.?\d*)%',
            r'å°±ä¸š.*?(\d+\.?\d*)%'
        ]
        for pattern in rate_patterns:
            match = re.search(pattern, text)
            if match:
                data['employment_rate'] = float(match.group(1))
                break
        
        # åŒ¹é…ç­¾çº¦ç‡
        signing_patterns = [
            r'ç­¾çº¦ç‡[:ï¼š]?(\d+\.?\d*)%',
            r'ç­¾(?:çº¦|ä¸‰æ–¹)[^%]*(\d+\.?\d*)%'
        ]
        for pattern in signing_patterns:
            match = re.search(pattern, text)
            if match:
                data['signing_rate'] = float(match.group(1))
                break
        
        return data
    
    def extract_search_results(self, html: str, engine: str) -> List[str]:
        """ä»æœç´¢ç»“æœé¡µé¢æå–URLé“¾æ¥"""
        urls = []
        
        try:
            if engine == 'bing':
                soup = BeautifulSoup(html, 'lxml')
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if (href and href.startswith('http') 
                        and 'bing.com' not in href 
                        and 'microsoft.com' not in href
                        and 'live.com' not in href
                        and 'msn.com' not in href):
                        urls.append(href)
            elif engine == 'sogou':
                # æœç‹—ä½¿ç”¨åŠ¨æ€æ¸²æŸ“ï¼Œéœ€è¦ä» HTML æºç çš„ JSON æ•°æ®ä¸­æå– URL
                # æœç´¢ç»“æœé“¾æ¥é€šå¸¸åœ¨ sup_url å­—æ®µä¸­
                patterns = [
                    r'\"sup_url\":\"(https?:\\\\/\\\\/[^\"]+)\"',
                    r'\"url\":\"(https?:\\\\/\\\\/[^\"]+)\"',
                    r'\"link\":\"(https?:\\\\/\\\\/[^\"]+)\"'
                ]
                
                for pattern in patterns:
                    matches = re.findall(pattern, html)
                    for url in matches:
                        clean_url = url.replace('\\\\/', '/')
                        url_lower = clean_url.lower()
                        
                        # è¿‡æ»¤æ¡ä»¶
                        if (len(clean_url) > 40 and 
                            'sogou.com' not in url_lower and 
                            'sogoucdn.com' not in url_lower and
                            'sogouws.com' not in url_lower and
                            'openapi' not in url_lower and  # æ’é™¤ API é“¾æ¥
                            'qpic.cn' not in url_lower):  # æ’é™¤ QQ å›¾ç‰‡
                            urls.append(clean_url)
                
                # å¦‚æœæ²¡æœ‰ä» JSON ä¸­æå–åˆ°ï¼Œå°è¯•ä» href å±æ€§ä¸­æå–ï¼ˆPC ç‰ˆï¼‰
                if not urls:
                    soup = BeautifulSoup(html, 'lxml')
                    for link in soup.find_all('a', href=True):
                        href = link['href']
                        if (href and href.startswith('http') 
                            and 'sogou.com' not in href 
                            and 'sogoucdn.com' not in href
                            and 'sogouws.com' not in href):
                            urls.append(href)
            elif engine == 'sogou':
                # æœç‹—ä½¿ç”¨åŠ¨æ€æ¸²æŸ“ï¼Œéœ€è¦ä» HTML æºç çš„ JSON æ•°æ®ä¸­æå– URL
                # æœç´¢ç»“æœé“¾æ¥é€šå¸¸åœ¨ sup_url å­—æ®µä¸­
                patterns = [
                    r'\"sup_url\":\"(https?:\\\\/\\\\/[^\"]+)\"',
                    r'\"url\":\"(https?:\\\\/\\\\/[^\"]+)\"',
                    r'\"link\":\"(https?:\\\\/\\\\/[^\"]+)\"'
                ]
                
                for pattern in patterns:
                    matches = re.findall(pattern, html)
                    for url in matches:
                        clean_url = url.replace('\\\\/', '/')
                        url_lower = clean_url.lower()
                        
                        # è¿‡æ»¤æ¡ä»¶
                        if (len(clean_url) > 40 and 
                            'sogou.com' not in url_lower and 
                            'sogoucdn.com' not in url_lower and
                            'sogouws.com' not in url_lower and
                            'openapi' not in url_lower and  # æ’é™¤ API é“¾æ¥
                            'qpic.cn' not in url_lower):  # æ’é™¤ QQ å›¾ç‰‡
                            urls.append(clean_url)
                
                # å¦‚æœæ²¡æœ‰ä» JSON ä¸­æå–åˆ°ï¼Œå°è¯•ä» href å±æ€§ä¸­æå–ï¼ˆPC ç‰ˆï¼‰
                if not urls:
                    soup = BeautifulSoup(html, 'lxml')
                    for link in soup.find_all('a', href=True):
                        href = link['href']
                        if (href and href.startswith('http') 
                            and 'sogou.com' not in href 
                            and 'sogoucdn.com' not in href
                            and 'sogouws.com' not in href):
                            urls.append(href)
        except Exception as e:
            print(f"   âš ï¸ æå–æœç´¢ç»“æœæ—¶å‡ºé”™: {e}")
        
        return urls
    
    def filter_urls(self, urls: List[str]) -> List[str]:
        """è¿‡æ»¤URLï¼Œå»é™¤å¹¿å‘Šå’Œä¸ç›¸å…³é“¾æ¥"""
        filtered = []
        
        # å®šä¹‰éœ€è¦è¿‡æ»¤çš„åŸŸå
        blocked_domains = [
            'ads.', 'advertisement', 'ad.', 'tracking.',
            'youdao', 'hao123', 'sohu.com', '163.com',
            '360.cn', 'tongji.baidu.com',
            'sogoucdn.com', 'sogouws.com',
            'baiducontent.com', 'm.baidu.com', 'baidubce.com',
            'play.google.com', 'apps.apple.com',
            'facebook.com', 'twitter.com', 'instagram.com',
            'linkedin.com', 'youtube.com', 'tiktok.com',
            'ifsc', 'swiftcode', 'ifsccode', 'ifsccodebank',
            'cleartax', 'getswipe'
        ]
        
        # å®šä¹‰éœ€è¦ä¿ç•™çš„å…³é”®è¯ï¼ˆæ›´å¹¿æ³›ï¼‰
        keep_keywords = [
            'edu.cn', 'gov.cn', 'org.cn', 'ac.cn',
            'news', 'xinwen', 'zaixian', 'article',
            'employment', 'job', 'zhipin', 'jobui',
            'career', 'graduate', 'bysh',
            'wangjiao', 'juye', 'qiuzhi',
            'rencai', 'zhaopin', '51job',
            'chsi', 'moe', 'people.com.cn',
            'chinanews', 'xinhuanet', 'thepaper',
            'cctv', 'cnbeta', '36kr',
            'gaoxiaojob', 'yjbys',
            'paper', 'report', 'analysis',
            'data', 'statistics', 'trend'
        ]
        
        for url in urls:
            url_lower = url.lower()
            
            # è¿‡æ»¤å¹¿å‘ŠåŸŸå
            if any(blocked in url_lower for blocked in blocked_domains):
                continue
            
            # è¿‡æ»¤çŸ­URLï¼ˆå¯èƒ½æ˜¯é‡å®šå‘æˆ–å¹¿å‘Šï¼‰
            if len(url) < 30:
                continue
            
            # è¿‡æ»¤é‡å¤çš„ä¸»é¡µ
            if url.endswith('/') and len(url) < 35:
                continue
            
            # è¿‡æ»¤æ˜æ˜¾ä¸ç›¸å…³çš„URLï¼ˆIFSCä»£ç ã€é“¶è¡Œç­‰ï¼‰
            if 'ifsc' in url_lower or 'swiftcode' in url_lower:
                continue
            
            # ä¿ç•™åŒ…å«å…³é”®è¯çš„URL
            if any(keyword in url_lower for keyword in keep_keywords):
                filtered.append(url)
            else:
                # å¦‚æœURLçœ‹èµ·æ¥æ˜¯å†…å®¹é¡µé¢ï¼Œä¹Ÿä¿ç•™
                if (len(url) > 50 and 
                    any(char in url for char in ['?', '=', '-', '_', '.', '/']) and
                    len(url.split('/')) > 3):
                    filtered.append(url)
        
        return filtered
    
    def search_bing(self, query: str, max_pages: int = 3, results_per_page: int = 10) -> List[str]:
        """ä½¿ç”¨ Bing æœç´¢å¼•æ“æœç´¢ï¼ˆæ”¯æŒç¿»é¡µï¼‰"""
        print(f"\nğŸ” Bing æœç´¢: {query}")
        search_url = "https://www.bing.com/search"
        all_urls = []
        
        for page in range(1, max_pages + 1):
            offset = (page - 1) * results_per_page
            params = {
                'q': query,
                'count': results_per_page,
                'first': offset,
                'setlang': 'zh-CN'
            }
            
            print(f"   ç¬¬ {page} é¡µ (offset: {offset})")
            html = self.fetch_page(search_url, params=params)
            if html:
                urls = self.extract_search_results(html, 'bing')
                print(f"   æ‰¾åˆ° {len(urls)} ä¸ªç»“æœ")
                all_urls.extend(urls)
            else:
                print(f"   ç¬¬ {page} é¡µæŠ“å–å¤±è´¥")
                break
            
            time.sleep(1)
        
        return all_urls
    
    def search_sogou(self, query: str, max_pages: int = 3, results_per_page: int = 10) -> List[str]:
        """ä½¿ç”¨æœç‹—æœç´¢å¼•æ“æœç´¢ï¼ˆæ”¯æŒç¿»é¡µï¼‰"""
        print(f"\nğŸ” æœç‹—æœç´¢: {query}")
        search_url = "https://sogou.com/web"
        all_urls = []
        
        for page in range(max_pages):
            page_num = page + 1
            params = {
                'query': query,
                'page': page_num,
                'ie': 'utf8'
            }
            
            print(f"   ç¬¬ {page_num} é¡µ")
            html = self.fetch_page(search_url, params=params)
            
            if html:
                urls = self.extract_search_results(html, 'sogou')
                print(f"   æ‰¾åˆ° {len(urls)} ä¸ªç»“æœ")
                all_urls.extend(urls)
            else:
                print(f"   ç¬¬ {page_num} é¡µæŠ“å–å¤±è´¥")
                break
            
            time.sleep(1)
        
        return all_urls
    
    def search_and_scrape(self, query: str, max_pages: int = 3, num_to_scrape: int = 30) -> List[Dict[str, Any]]:
        """æœç´¢å¹¶æŠ“å–ç›¸å…³é¡µé¢æ•°æ®ï¼ˆæ”¯æŒç¿»é¡µï¼‰"""
        all_results = []
        
        print(f"\n{'='*60}")
        print(f"å¼€å§‹æœç´¢å¹¶æŠ“å–æ•°æ®: {query}")
        print(f"ç¿»é¡µ: æœ€å¤š {max_pages} é¡µ/æœç´¢å¼•æ“")
        print(f"æŠ“å–: æœ€å¤š {num_to_scrape} ä¸ªé¡µé¢")
        print(f"{'='*60}")
        
        # åŒæ—¶ä½¿ç”¨ Bing å’Œæœç‹—æœç´¢ï¼ˆæ”¯æŒç¿»é¡µï¼‰
        bing_urls = self.search_bing(query, max_pages=max_pages, results_per_page=10)
        sogou_urls = self.search_sogou(query, max_pages=max_pages, results_per_page=10)
        
        print(f"\nğŸ“Š æœç´¢ç»Ÿè®¡:")
        print(f"   Bing æ‰¾åˆ°: {len(bing_urls)} ä¸ªç»“æœ")
        print(f"   æœç‹—æ‰¾åˆ°: {len(sogou_urls)} ä¸ªç»“æœ")
        
        # åˆå¹¶å»é‡
        all_urls = list(set(bing_urls + sogou_urls))
        
        # è¿‡æ»¤å¹¿å‘Šå’Œä¸ç›¸å…³é“¾æ¥
        all_urls = self.filter_urls(all_urls)
        
        print(f"   è¿‡æ»¤å: {len(all_urls)} ä¸ªå”¯ä¸€é“¾æ¥")
        
        if not all_urls:
            print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆæœç´¢ç»“æœ")
            return all_results
        
        # é™åˆ¶æŠ“å–æ•°é‡
        urls_to_scrape = all_urls[:num_to_scrape]
        print(f"\nå¼€å§‹æŠ“å– {len(urls_to_scrape)} ä¸ªé¡µé¢...")
        
        # æŠ“å–æœç´¢ç»“æœé¡µé¢
        success_count = 0
        for idx, url in enumerate(urls_to_scrape, 1):
            print(f"\n[{idx}/{len(urls_to_scrape)}] æŠ“å–: {url}")
            html = self.fetch_page(url)
            if html:
                data = self.extract_employment_data(html)
                data['source_url'] = url
                data['search_query'] = query
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
                has_data = (data.get('employment_rate', 0) > 0 or 
                          data.get('signing_rate', 0) > 0 or 
                          data.get('total_graduates'))
                
                if has_data or len(html) > 1000:  # æœ‰æ•°æ®æˆ–é¡µé¢å†…å®¹å……è¶³
                    all_results.append(data)
                    success_count += 1
                    print(f"   âœ… æˆåŠŸ (å°±ä¸šç‡: {data.get('employment_rate', 0)}%)")
                else:
                    print(f"   âš ï¸ é¡µé¢æ•°æ®ä¸è¶³")
            else:
                print(f"   âŒ æŠ“å–å¤±è´¥")
            
            time.sleep(2)
        
        print(f"\nâœ… æˆåŠŸæŠ“å– {success_count} ä¸ªæœ‰æ•ˆé¡µé¢")
        return all_results
    
    def scrape_multiple_sources(self, urls: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡æŠ“å–å¤šä¸ªæ•°æ®æº"""
        results = []
        for url in urls:
            print(f"æ­£åœ¨æŠ“å–: {url}")
            html = self.fetch_page(url)
            if html:
                data = self.extract_employment_data(html)
                data['source_url'] = url
                results.append(data)
            time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«
        return results


class DataScraperTool:
    """æ•°æ®æŠ“å–å·¥å…·ç±»"""
    
    def __init__(self):
        self.scraper = WebScraper()
        
        self.search_queries = [
            '2024å¹´ é«˜æ ¡æœ¬ç§‘æ¯•ä¸šç”Ÿ å°±ä¸šç‡',
            '2024-2025 å¤§å­¦ç”Ÿ å°±ä¸šæ•°æ® ç»Ÿè®¡',
            '2024å±Š æœ¬ç§‘ç”Ÿ å°±ä¸šæƒ…å†µ æŠ¥å‘Š',
            'é«˜æ ¡ æ¯•ä¸šç”Ÿ ç­¾çº¦ç‡ 2024',
            'å¤§å­¦ç”Ÿ å°±ä¸šè¶‹åŠ¿ 2024 2025',
            'é«˜æ ¡æ¯•ä¸šç”Ÿå°±ä¸šè´¨é‡æŠ¥å‘Š 2024',
            'æœ¬ç§‘ç”Ÿå°±ä¸šæ•°æ® 2024å¹´'
        ]
    
    def scrape_employment_data(self) -> str:
        """æŠ“å–å°±ä¸šæ•°æ®ä¸»å‡½æ•° - ä½¿ç”¨æœç´¢å¼•æ“æœç´¢ï¼ˆæ”¯æŒç¿»é¡µï¼‰"""
        all_data = []
        
        print("\n" + "="*70)
        print("ã€æ•°æ®æŠ“å–ç­–ç•¥ã€‘ä½¿ç”¨æœç´¢å¼•æ“æœç´¢ï¼Œæ”¯æŒç¿»é¡µï¼ˆæœ€å¤š5é¡µ/æœç´¢å¼•æ“ï¼‰")
        print("="*70)
        
        # ä½¿ç”¨æœç´¢å¼•æ“æœç´¢å¹¶ç¿»é¡µ
        for query in self.search_queries[:5]:  # ä½¿ç”¨å‰5ä¸ªæŸ¥è¯¢
            results = self.scraper.search_and_scrape(
                query, 
                max_pages=5,  # æ¯ä¸ªæœç´¢å¼•æ“æœ€å¤šç¿»5é¡µ
                num_to_scrape=30  # å–30ä¸ªé¡µé¢è¿›è¡ŒæŠ“å–
            )
            all_data.extend(results)
            time.sleep(3)  # æŸ¥è¯¢é—´å»¶è¿Ÿ
        
        # ç»Ÿè®¡æ±‡æ€»
        summary = self._summarize_data(all_data)
        return json.dumps(summary, ensure_ascii=False, indent=2)
    
    def _summarize_data(self, data_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ±‡æ€»æ•°æ®"""
        summary = {
            'total_sources': len(data_list),
            'employment_rates': [],
            'signing_rates': [],
            'graduate_counts': [],
            'sources': []
        }
        
        for data in data_list:
            if data.get('employment_rate'):
                summary['employment_rates'].append(data['employment_rate'])
            if data.get('signing_rate'):
                summary['signing_rates'].append(data['signing_rate'])
            if data.get('total_graduates'):
                summary['graduate_counts'].append(data['total_graduates'])
            summary['sources'].append(data.get('source_url', ''))
        
        # è®¡ç®—å¹³å‡å€¼
        if summary['employment_rates']:
            summary['avg_employment_rate'] = sum(summary['employment_rates']) / len(summary['employment_rates'])
        if summary['signing_rates']:
            summary['avg_signing_rate'] = sum(summary['signing_rates']) / len(summary['signing_rates'])
        
        return summary
